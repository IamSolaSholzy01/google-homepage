{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg4BK9dsLHHQ"
      },
      "source": [
        "### Dataset Description\n",
        "\n",
        "In this competition your task is to predict whether a passenger was transported to an alternate dimension during the Spaceship Titanic's collision with the spacetime anomaly. To help you make these predictions, you're given a set of personal records recovered from the ship's damaged computer system.\n",
        "\n",
        "### File and Data Field Descriptions:\n",
        "\n",
        "    <>train.csv - Personal records for about two-thirds (~8700) of the passengers, to be used as training data.\n",
        "    <>PassengerId - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is  their number within the group. People in a group are often family members, but not always.\n",
        "    <>HomePlanet - The planet the passenger departed from, typically their planet of permanent residence.\n",
        "    <>CryoSleep - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
        "    <>Cabin - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
        "    <>Destination - The planet the passenger will be debarking to.\n",
        "    <>Age - The age of the passenger.\n",
        "    <>VIP - Whether the passenger has paid for special VIP service during the voyage.\n",
        "    <>RoomService, FoodCourt, ShoppingMall, Spa, VRDeck - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
        "    <>Name - The first and last names of the passenger.\n",
        "    <>Transported - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n",
        "    \n",
        "test.csv - Personal records for the remaining one-third (~4300) of the passengers, to be used as test data. Your task is to predict the value of Transported for the passengers in this set.\n",
        "\n",
        "sample_submission.csv - A submission file in the correct format.\n",
        "PassengerId - Id for each passenger in the test set.\n",
        "Transported - The target. For each passenger, predict either True or False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RbhuqWF8LHHV"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Data wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Data visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# File\n",
        "import os\n",
        "\n",
        "\n",
        "# Remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aYpAR6YpLHHX",
        "outputId": "9a8d948c-830c-4d1f-9e64-8b29283f74c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fea5a8dafc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
          ]
        }
      ],
      "source": [
        "# Import Files\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvRFxtM_LHHY"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "train.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s13NCu4LHHY"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "test.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d31487cLHHZ"
      },
      "outputs": [],
      "source": [
        "# Inspect Data\n",
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioJUJQJZLHHa"
      },
      "outputs": [],
      "source": [
        "# Inspect Data\n",
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JDKtZpMLHHa"
      },
      "outputs": [],
      "source": [
        "#Print shape of Dataset\n",
        "print(f'The shape of train data is: \\n', train.shape)\n",
        "print(f'The shape of test data is: \\n', test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoKC9DoyLHHb"
      },
      "outputs": [],
      "source": [
        "# Function to extract cabin grp and side \n",
        "def cabin_split(x):\n",
        "    ## try and exception is used to navigate through the nan values\n",
        "    try:\n",
        "        u= x.split('/')\n",
        "        return str(u[0] + u[2])\n",
        "    except AttributeError as e:\n",
        "        return x\n",
        "\n",
        "# Extract cabin grp and side\n",
        "for data in [test, train]:\n",
        "    data[\"Cabin_grp\"] = data.Cabin.apply(cabin_split)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvm-MXa6LHHd"
      },
      "outputs": [],
      "source": [
        "# Function fill the Cabin_grp\n",
        "for data in [train, test]:\n",
        "    data.Cabin_grp.fillna(method=\"pad\", inplace=True)\n",
        "\n",
        "# Function to Fill the Cabin from the Carbin_grp    \n",
        "def fill_cabin(data):\n",
        "    ## try and exception is used to navigate through the nan values\n",
        "    try:\n",
        "        a = data.str.split('')\n",
        "        return str(a[1] + \"/\" + str(np.random.choice(a=1500, size=1)[0])+ \"/\" + a[2])\n",
        "    except AttributeError as e:\n",
        "        return data\n",
        "\n",
        "# Fill the Cabin from the Carbin_grp\n",
        "for data in [test, train]:\n",
        "    for index, value in enumerate(list(data.Cabin.isna())):\n",
        "        if value:\n",
        "            data[\"Cabin\"].iloc[index]= data.Cabin_grp.apply(fill_cabin).iloc[index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7CqgqaKLHHd"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values\n",
        "train.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nac01O8sLHHe"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values\n",
        "test.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfBm_wpLLHHf"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "train.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOMMtiDsLHHf"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "test.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhfyvmGZLHHg"
      },
      "source": [
        "### EXPLOROTARY DATA ANALYSIS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZTmYGIiLHHg"
      },
      "source": [
        "#### Categorical Variables : HomePlanet, CryoSleep, Destination,VIP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHnQRYqNLHHg"
      },
      "source": [
        "##### 1. HomePlanet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL9t1QflLHHh"
      },
      "outputs": [],
      "source": [
        "train.HomePlanet.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-Toxw2LHHh"
      },
      "outputs": [],
      "source": [
        "#Mean \"Transported\" by HomePlanet\n",
        "\n",
        "train[['HomePlanet', 'Transported']].groupby('HomePlanet', as_index=False).mean().sort_values(by= 'Transported', ascending=False)\n",
        "\n",
        "# Europa, Mars, Earth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poYk7AiMLHHh"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "sns.barplot(x = 'HomePlanet', y ='Transported', data = train)\n",
        "plt.ylabel('Transported Probability')\n",
        "plt.title('Transported Probability by Home Planet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-2jjWFLHHh"
      },
      "source": [
        "##### 2. Destination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5oJfjpzLHHi"
      },
      "outputs": [],
      "source": [
        "train['Destination'].value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x85_1Xi4LHHi"
      },
      "outputs": [],
      "source": [
        "# Mean \"Transported\" by Destination\n",
        "train[[\"Destination\", \"Transported\"]].groupby('Destination', as_index=False).mean().sort_values(by=\"Transported\", ascending=False)\n",
        "\n",
        "# 55 Cancri e, PSO J318.5-22, TRAPPIST-1e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkidicnwLHHi"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "sns.barplot(x=\"Destination\", y=\"Transported\", data=train)\n",
        "plt.ylabel('Transported Probability')\n",
        "plt.title('Transported Probability by Destination')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnjzdoQ3LHHi"
      },
      "source": [
        "##### 3. CryoSleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkxBYVbBLHHj"
      },
      "outputs": [],
      "source": [
        "train.CryoSleep.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g76GCU7yLHHj"
      },
      "outputs": [],
      "source": [
        "# Mean \"Transpored\" by CryoSleep \n",
        "\n",
        "train[['CryoSleep', 'Transported']].groupby(\"CryoSleep\", as_index=False).mean().sort_values(by=\"Transported\", ascending=False)\n",
        "\n",
        "# True, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C9zTprjLHHj"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "sns.barplot(x=\"CryoSleep\", y=\"Transported\", data=train)\n",
        "plt.xlabel(\"Transported Probability\")\n",
        "plt.ylabel(\"Transported Probability by CryoSleep\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxuMAhX7LHHj"
      },
      "source": [
        "##### 4. VIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OELU_1WpLHHj"
      },
      "outputs": [],
      "source": [
        "train.VIP.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_W29-PZqLHHk"
      },
      "outputs": [],
      "source": [
        "# Mean \"Transported\" by VIP \n",
        "\n",
        "train[['VIP', 'Transported']].groupby(\"VIP\", as_index=False).mean().sort_values(by=\"Transported\", ascending=False)\n",
        "\n",
        "# False, True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQZFIV6qLHHk"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "sns.barplot(x=\"VIP\", y=\"Transported\", data=train)\n",
        "plt.xlabel(\"Transported Probability\")\n",
        "plt.ylabel(\"Transported Probability by VIP\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flxNmyXaLHHl"
      },
      "source": [
        "### FILL MISSING TEST VALUES\n",
        "\n",
        "##### From the EDA performed earlier by Rasheed, he defined the functions below for filling missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0crAI431LHHl"
      },
      "outputs": [],
      "source": [
        "# For Functions for Missing Values\n",
        "def fill_missing_1(data, target_column: str, cond_column1: str, cond_column2: str, cond_value1: str, cond_value2, fill):\n",
        "    common= data[target_column].isna()\n",
        "    condition= [(data[cond_column1]>= cond_value1) & (data[cond_column2]== cond_value2) & (common)]\n",
        "    fill_with= [fill]\n",
        "    data[target_column]= np.select(condition, fill_with, default= data[target_column].values)\n",
        "\n",
        "def fill_missing_2(data, target_column: str, cond_column: str, cond_value:int, fill):\n",
        "    common= data[target_column].isna()\n",
        "    cond= [(data[cond_column] <= cond_value) &(common)]\n",
        "    fill_with= [fill]\n",
        "    data[target_column]= np.select(cond, fill_with, default= data[target_column].values)\n",
        "\n",
        "def fill_missing_3(data, target_column: str, cond_column1: str, cond_column2: str, cond_value1: str, cond_value2, fill):\n",
        "    common= data[target_column].isna()\n",
        "    condition= [(data[cond_column1]== cond_value1) & (data[cond_column2]== cond_value2) & (common)]\n",
        "    fill_with= [fill]\n",
        "    data[target_column]= np.select(condition, fill_with, default= data[target_column].values)\n",
        "\n",
        "def fill_missing_4(data, target_column: str, cond_column1: str,  cond_value1: str, fill):\n",
        "    common= data[target_column].isna()\n",
        "    condition= [(data[cond_column1]== cond_value1)  & (common)]\n",
        "    fill_with= [fill]\n",
        "    data[target_column]= np.select(condition, fill_with, default= data[target_column].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G14H28yZLHHl"
      },
      "outputs": [],
      "source": [
        "# Check if both train and test datas as same number of unique carbon_grp values\n",
        "\n",
        "len(train.Cabin_grp.unique()) == len(test.Cabin_grp.unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTO3RjgqLHHm"
      },
      "outputs": [],
      "source": [
        "# Fill: For ages greater than 40 and cabin_grp AP,BP, BS, CS , CP HomePlanet is Europa\n",
        "# Fill: For ages greater than 40  and cabin_grp GS, GP homeplanet is Earth\n",
        "for grp in [\"AP\",\"BP\", \"BS\", \"CS\" , \"CP\", \"GS\", \"GP\"]:\n",
        "    if grp in [\"GS\", \"GP\"]:\n",
        "        fill_missing_1(train, 'HomePlanet', \"Age\", 'Cabin_grp', 40, grp, 'Earth')\n",
        "        fill_missing_1(test, 'HomePlanet', \"Age\", 'Cabin_grp', 40, grp, 'Earth')\n",
        "    else:\n",
        "        fill_missing_1(train, 'HomePlanet', \"Age\", 'Cabin_grp', 40, grp, 'Europa')\n",
        "        fill_missing_1(test, 'HomePlanet', \"Age\", 'Cabin_grp', 40, grp, 'Europa')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ysxNSomLHHm"
      },
      "outputs": [],
      "source": [
        "# Fill for Shopmall and VIP sujected to Age 12 and 20 respectively\n",
        "for data in [train, test]:\n",
        "    fill_missing_2(data, 'ShoppingMall', 'Age', 12, 0)\n",
        "    fill_missing_2(data, 'VIP', 'Age', 20, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDqZ7GX-LHHm"
      },
      "outputs": [],
      "source": [
        "# Fill Missing Values (Contd)\n",
        "for data in [train, test]:\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'ES', 'TRAPPIST-1e', 'Mars')\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'ES', 'PSO J318.5-22', 'Earth')\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'ES', '55 Cancri e', 'Europa')\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'ES', '55 Cancri e', 'Europa')\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'DS', '55 Cancri e', 'Europa')\n",
        "    fill_missing_3(data, 'HomePlanet', 'Cabin_grp', 'Destination', 'DP', '55 Cancri e', 'Europa')\n",
        "\n",
        "\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'AS', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'AP', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'BS', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'BP', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'CS', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'CP', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'TP', 'Europa')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'FS', 'Earth')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'GS', 'Earth')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'GP', 'Earth')\n",
        "    fill_missing_4(data, 'HomePlanet', 'Cabin_grp', 'EP', 'Earth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdpPU_D_LHHn"
      },
      "outputs": [],
      "source": [
        "# Fill Missing Values (Contd)\n",
        "for data in [train, test]:\n",
        "    data['HomePlanet']= data['HomePlanet'].fillna('Mars')\n",
        "\n",
        "    fill_missing_4(data, 'CryoSleep', 'Cabin_grp', 'BS', True)\n",
        "    fill_missing_3(data, 'CryoSleep', 'Cabin_grp', 'Destination', 'GP', '55 Cancri e', True )\n",
        "    fill_missing_3(data, 'CryoSleep', 'Cabin_grp', 'Destination', 'GS', '55 Cancri e', True )\n",
        "    ## fill the remaining missing values with False\n",
        "    data['CryoSleep'] = data['CryoSleep'].fillna(False)\n",
        "\n",
        "    ## fill VIP the misiing values with False\n",
        "    data['VIP']= data['VIP'].fillna(False)\n",
        "\n",
        "    ## fill Destination with TRAPPIST-1e\n",
        "    data['Destination']= data['Destination'].fillna('TRAPPIST-1e')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6J2Lu0eQLHHn"
      },
      "outputs": [],
      "source": [
        "## Group by HomePlanet,  cabin_grp and destination then fill with median\n",
        "\n",
        "for data in [train, test]:\n",
        "    for col in ['Spa', 'VRDeck', 'ShoppingMall', 'RoomService', 'Age', 'FoodCourt']:\n",
        "        data[col] = data.groupby(['HomePlanet','Cabin_grp', 'Destination'])[col].apply(lambda x: x.fillna(x.median()))\n",
        "\n",
        "# Fill the remaining Nan Values:\n",
        "\n",
        "for data in [train, test]:\n",
        "    for col in ['Spa', 'VRDeck', 'ShoppingMall', 'RoomService', 'Age', 'FoodCourt']:\n",
        "        median = data[col].median()\n",
        "        data[col].fillna(value=median, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzoTdWqfLHHn"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values\n",
        "train.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9qIt2G0LHHn"
      },
      "outputs": [],
      "source": [
        "# Check for NaN values\n",
        "test.isnull().sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb9neIQrLHHo"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "train.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uTd1HlKLHHo"
      },
      "outputs": [],
      "source": [
        "# Sample Data\n",
        "test.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw1zoRqELHHo"
      },
      "outputs": [],
      "source": [
        "# Print out shape of datasets\n",
        "print(f'The shape of train data is: \\n', train.shape)\n",
        "print(f'The shape of test data is: \\n', test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g39z0YSdLHHo"
      },
      "source": [
        "#### Save to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1QJPcjMLHHp"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "# Function to make directory and create file\n",
        "def create_csv(filename:str, data:DataFrame):\n",
        "    directory = \"Wrangled_Data/\"\n",
        "    path = os.path.join(directory, filename)\n",
        "\n",
        "    # Make directory if directory doesn't exist\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "    # Check if file exists and delete\n",
        "    if os.path.exists(path):\n",
        "        os.remove(path)\n",
        "\n",
        "    # Read Dataframe to csv    \n",
        "    data.to_csv(path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv5YVfAsLHHp"
      },
      "outputs": [],
      "source": [
        "create_csv(\"train_new.csv\", train)\n",
        "create_csv(\"test_new.csv\", test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "574519ca09b26aca22fcbd258939ca899adfc49e01be43018cd5e5ed113a2a51"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}